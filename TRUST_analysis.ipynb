{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A template jupyter notebook for TRUST analysis\n",
    "Jeff Stout BCH 20201211\n",
    "\n",
    "Required python libraries (see requirments.txt for exact virtualenv versions):\n",
    "matplotlib\n",
    "numpy\n",
    "roipoly\n",
    "PyQt5\n",
    "nibabel\n",
    "pandas\n",
    "statsmodels\n",
    "json\n",
    "T2toSvO2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paramters\n",
    "N_bright_voxels = 3 # voxels in the roi that are averaged for each measurement\n",
    "# T1_blood = 1.818 # seconds  for SCD (Vaclavu)\n",
    "# print('T1 blood assumed to be ', T1_blood, 'for SCD from Vaclavu')\n",
    "T1_blood = 1.613 # seconds  original correction from Lu\n",
    "print('T1 blood assumed to be ', T1_blood, 'for SCD from Lu')\n",
    "\n",
    "# Hct = 0.35 # Hct is retrieved from the json sidecar below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the files and paths\n",
    "# Assuming a BIDS like organizational structure, see README_TRUST_PCMRI_organization.txt\n",
    "\n",
    "# Results \"derivatives\" directory:\n",
    "# results_dir = '/home/jeff/research/SCD/derivatives/trust/'\n",
    "results_dir = '/home/jeff/jeff.stout/research/PBI/derivatives/trust'\n",
    "\n",
    "# Numeric results written to: filename = os.path.join(results_dir, 'TRUST_analysis_results.csv')\n",
    "write_fields = False # when the csv is written include the fields in the first row?\n",
    "write_results = True # do you want to write the results to the csv?\n",
    "\n",
    "# Set up paths and output directory\n",
    "subject_str = 'PBI027'\n",
    "ses_str = 'ses-01'\n",
    "outdir = os.path.join(results_dir , subject_str, ses_str) # output directory this this subject\n",
    "\n",
    "# Data directory\n",
    "parentdir = os.path.join( '/home/jeff/jeff.stout/research/PBI/NIFTI', subject_str, ses_str) # data input directory for this subject\n",
    "\n",
    "fnamebase= subject_str + '_' +  ses_str + '_'\n",
    "\n",
    "# !! see \"Manual motion removal\" below for the cell to control motion rejection !!\n",
    "# Run this script once, look at the results then update that cell appropraitely\n",
    "# certainly a better way to do this, but I don't know, probably embeding the motion rejection the json sidecar is the right way to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files in the TRUST directory\n",
    "import re \n",
    "import json\n",
    "path2dat = os.path.join(parentdir, 'trust')\n",
    "# Get list of .nii files in the source data\n",
    "file_list = os.listdir(path2dat)\n",
    "file_list_nii = [x for x in file_list if re.search('.nii.gz', x)]\n",
    "file_list_nii.sort() # this makes certain that the \"second series\" is meaningful\n",
    "print('The complete file list is:')\n",
    "print(*file_list_nii, sep='\\n')\n",
    "!mkdir -p $outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hct value from the json sidecar and compare for consistency check\n",
    "for ind, x in enumerate(file_list_nii):\n",
    "    json_path = os.path.join(path2dat, x[0:-7] + '.json')\n",
    "    print(json_path)\n",
    "    f = open (json_path, \"r\") \n",
    "    # Reading from file \n",
    "    data = json.loads(f.read()) \n",
    "    Hct = data['Hct']\n",
    "    # Closing file\n",
    "    f.close()\n",
    "    if ind is not 0:\n",
    "        assert Hct == Hct_temp, 'Hct value in inconsistent'\n",
    "    Hct_temp = Hct\n",
    "\n",
    "print('Hct as determined from json sidecar = ', Hct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there multiple runs?\n",
    "\n",
    "# A little list counter I found here:\n",
    "# (https://www.geeksforgeeks.org/python-program-to-count-duplicates-in-a-list-of-tuples/)\n",
    "def count(listOfTuple): \n",
    "    count_map = {} \n",
    "    for i in listOfTuple: \n",
    "        count_map[i] = count_map.get(i, 0) +1\n",
    "    print(count_map)\n",
    "    return count_map # this is a dictionary\n",
    "\n",
    "if file_list_nii[0].find('run') == -1:\n",
    "    print('\"run\" doesnt appear in the file name, so using only files without \"run\" in name,\\nthus removing from analysis:')\n",
    "    print(*[x for x in file_list_nii if \"run\" in x], sep='\\n')\n",
    "    file_list_nii = [x for x in file_list_nii if \"run\" not in x]\n",
    "    idx = file_list_nii[0].find('prep') # where is prep in the string, ACCORIDNG TO THE FIRST FILE IN THE LIST\n",
    "    preps = [z[idx:idx+5] for z in file_list_nii] # make list of preps\n",
    "    preps_list = list(set(preps)) # make list of unique preps\n",
    "    preps_list.sort()\n",
    "    preps_dict = dict.fromkeys(preps_list, None) # convert to dictionry to keep track of filename indexes below\n",
    "    N_runs = 1\n",
    "    runs = [1] * len(preps)\n",
    "else:\n",
    "    print('\"run\" appears in file names and the run counts are:')\n",
    "    idx = file_list_nii[0].find('prep') # where is prep in the string\n",
    "    preps = [z[idx:idx+5] for z in file_list_nii] # make list of preps\n",
    "    idx = file_list_nii[0].find('run')\n",
    "    runs = [z[idx:idx+5] for z in file_list_nii]\n",
    "    preps_list = list(set(preps)) # make list of unique preps\n",
    "    preps_list.sort()\n",
    "    preps_dict = dict.fromkeys(preps_list, None) # convert to dictionry to keep track of filename indexes below\n",
    "    prep_runs = count(preps)\n",
    "    N_runs = max(prep_runs.values())\n",
    "\n",
    "    # Convert run list of strings to list of int\n",
    "    runs = [int(sub[-1]) for sub in runs]\n",
    "    \n",
    "# I would like a list to index the preps, to go between prep# and filename(s)\n",
    "for x in preps_list:\n",
    "    idx = 0\n",
    "    temp_idx = []\n",
    "    for y in file_list_nii:\n",
    "        if re.search(x, y) is not None:\n",
    "            preps_dict[x] = temp_idx.append(int(idx))\n",
    "        idx += 1\n",
    "    preps_dict[x] = temp_idx\n",
    "\n",
    "# # This is the syntax for calling this cool little  prep# and filename(s) dictionary in the future:\n",
    "# for x in preps_dict['prep0']:\n",
    "#     print(x)\n",
    "#     print(file_list_nii[x])\n",
    "    \n",
    "# I would like TE vector for later\n",
    "prep_vec = []\n",
    "for x in preps:\n",
    "    prep_vec.append(int(x[4]))\n",
    "TE_vec = np.array(prep_vec) * 18e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't a lot of data so just load it all, it is loaded in the order given in file_list_nii\n",
    "temp_array = []\n",
    "for x in file_list_nii:\n",
    "    image = nib.load(os.path.join(path2dat , x))\n",
    "    temp_array.append(image.get_fdata())\n",
    "    print('Size of data in file', x, 'is', image.shape)\n",
    "\n",
    "image_block = np.array(temp_array) # this line will produce an error if there are different numbers of measurements in a single prep\n",
    "image_block = image_block.squeeze()\n",
    "image_block = np.transpose(image_block, (0, 3, 1, 2))\n",
    "\n",
    "# currently it is hard coded that the number acquistion in each run is 6 (3 meas, i.e. ctrl-tag pairs)\n",
    "N_meas = 3\n",
    "assert image_block.shape[1] == N_meas * 2, 'Measurements in the TRUST acquistion is not 3'\n",
    "\n",
    "# Do the subtraction\n",
    "sub_image_block = image_block[:,1::2,:,:] - image_block[:,::2,:,:]\n",
    "sub_image_block.flags.writeable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual motion removal\n",
    "motion_select = np.zeros([len(preps), N_meas])\n",
    "# motion_select[0,2]=1\n",
    "# motion_select[1,2]=1\n",
    "# motion_select[3,1]=1\n",
    "# motion_select[4,0]=1\n",
    "# motion_select[4,2]=1\n",
    "# motion_select[5,0]=1\n",
    "# motion_select[8,0]=1\n",
    "# SCD-O1\n",
    "# motion_select[4,0:3]=1\n",
    "\n",
    "## PBI motion select\n",
    "# PBI027\n",
    "motion_select[3,1]=1\n",
    "# PBI042\n",
    "# motion_select[0,0:3]=1\n",
    "# motion_select[1,1]=1\n",
    "# motion_select[4,1:3]=1\n",
    "# PBI083\n",
    "# motion_select[0,1]=1\n",
    "# motion_select[1,1]=1\n",
    "# motion_select[4,0]=1\n",
    "# motion_select[5,0]=1\n",
    "# motion_select[5,2]=1\n",
    "# PBI087\n",
    "# motion_select[4,2]=1\n",
    "\n",
    "## IVH motion select\n",
    "# B057\n",
    "# motion_select[2,2]=1\n",
    "# B061\n",
    "# motion_select[1,0]=1\n",
    "# motion_select[2,1]=1\n",
    "# motion_select[3,0]=1\n",
    "# motion_select[4,0]=1\n",
    "# B063\n",
    "# motion_select[0,0]=1\n",
    "# motion_select[0,1]=1\n",
    "# B074\n",
    "# motion_select[0,1]=1\n",
    "# motion_select[0,2]=1\n",
    "# motion_select[1,:]=1 # This prep had a difference spin label thickness for some reason\n",
    "# motion_select[6,0:2]=1\n",
    "\n",
    "#B026_ses-02\n",
    "# motion_select[1,0]=1\n",
    "\n",
    "print('motion_select =\\n', motion_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subtracted data figure\n",
    "\n",
    "fig = plt.figure(figsize=(9, 26))\n",
    "columns = N_meas\n",
    "rows = len(preps)\n",
    "\n",
    "# ax enables access to manipulate each of subplots\n",
    "ax = []\n",
    "ind = 1\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        img = sub_image_block[i,j,:,:].squeeze()\n",
    "        # create subplot and append to ax\n",
    "        ax.append( fig.add_subplot(rows, columns, ind) )\n",
    "        ax[-1].set_title('p' + preps[i][-1] + 'r' + str(runs[i]))  # set title\n",
    "        ax[-1].axis('off')\n",
    "        plt.gray()\n",
    "        plt.imshow(img)\n",
    "        ind += 1\n",
    "        \n",
    "plt.savefig(os.path.join(outdir, fnamebase + 'subtracted.png'))\n",
    "plt.show()  # finally, render the plot\n",
    "print('Image saved as: ', os.path.join(outdir, fnamebase + 'subtracted.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw ROI around SSS if an ROI doesn't already exist\n",
    "\n",
    "ROI_path = os.path.join(outdir, fnamebase + 'ROI.nii.gz');\n",
    "\n",
    "if os.path.exists(ROI_path):\n",
    "    img = nib.load(ROI_path)\n",
    "    sss_roi_bin = img.get_fdata().squeeze() == 1\n",
    "    print('ROI loaded from: ', ROI_path)\n",
    "    plt.imshow(sss_roi_bin)\n",
    "\n",
    "else:\n",
    "    # for this to work remotely the orginal ssh session with -X needs to be open\n",
    "    %matplotlib qt\n",
    "    plt.imshow(image_block[1,1,:,:])\n",
    "\n",
    "    from roipoly import RoiPoly\n",
    "    sss_roi = RoiPoly(color='r') # draw new ROI in red color\n",
    "    mask = sss_roi.get_mask(image_block[0,0,:,:]) # create binary signal mask\n",
    "    %matplotlib inline\n",
    "    plt.imshow(image_block[1,1,:,:]) \n",
    "    sss_roi.display_roi()\n",
    "    sss_roi_bin = sss_roi.get_mask(sub_image_block[1,1,:,:])\n",
    "    \n",
    "    # Save the SSS_mask as a nifti for future viewing\n",
    "    img = nib.load( os.path.join(path2dat, file_list_nii[1]) )\n",
    "    aff = img.header.get_qform()\n",
    "    img = nib.Nifti1Image(sss_roi_bin.astype('int16'), aff)\n",
    "    nib.save(img, ROI_path)\n",
    "    print('ROI saved to nifti: ', ROI_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors of mean three brightest voxels and TEs\n",
    "%matplotlib inline\n",
    "\n",
    "# This function takes in a masked image and gives you back the three brightest voxels,\n",
    "# their indicies, and a bright voxel mask\n",
    "# Note: for multiple max voxels of identical SI, \"np.argmax: In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\"\n",
    "def findbrightvox(image, N_bright_voxels = 3):\n",
    "    voxel_vals = []\n",
    "    val_indicies = []\n",
    "    brtvox_mask = np.zeros(image.shape, dtype=np.bool)\n",
    "    for x in range(N_bright_voxels):\n",
    "        ind = np.unravel_index(np.argmax(image, axis=None), image.shape) # find the index of the max val in the image\n",
    "        val_indicies.append(ind)\n",
    "        voxel_vals.append(image[ind]) # record the max val\n",
    "        image[ind] = 0 # eliminate this max vox for the next iteration\n",
    "        brtvox_mask[ind] = True # create the bright voxel mask\n",
    "    return voxel_vals, val_indicies, brtvox_mask\n",
    "\n",
    "# now get the three brightest voxels for each subtracted acquisition\n",
    "# sub_image_block.shape => (10, 3, 70, 70)\n",
    "voxel_vals = np.zeros([sub_image_block.shape[0],sub_image_block.shape[1],N_bright_voxels])\n",
    "brtvox_mask = sub_image_block.copy()\n",
    "for i in range(sub_image_block.shape[0]):\n",
    "    for j in range(sub_image_block.shape[1]):\n",
    "        [v, ind, m] = findbrightvox(sss_roi_bin*sub_image_block[i,j,:,:], N_bright_voxels)\n",
    "        voxel_vals[i,j,:] = v\n",
    "        brtvox_mask[i,j,:,:] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bright voxel mask figure\n",
    "\n",
    "fig = plt.figure(figsize=(9, 26))\n",
    "columns = N_meas\n",
    "rows = len(preps)\n",
    "\n",
    "# ax enables access to manipulate each of subplots\n",
    "ax = []\n",
    "ind = 1\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        img = brtvox_mask[i,j,:,:].squeeze()\n",
    "        # create subplot and append to ax\n",
    "        ax.append( fig.add_subplot(rows, columns, ind) )\n",
    "        ax[-1].set_title('p' + preps[i][-1] + 'r' + str(runs[i]))  # set title\n",
    "        ax[-1].axis('off')\n",
    "        plt.gray()\n",
    "        plt.imshow(img)\n",
    "        ind += 1\n",
    "        \n",
    "plt.savefig(os.path.join(outdir, fnamebase + 'brtvoxmask.png'))\n",
    "plt.show()  # finally, render the plot\n",
    "print('Image saved as: ', os.path.join(outdir, fnamebase + 'brtvoxmask.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of three brightest voxels vs TE\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(TE_vec, np.mean(voxel_vals, axis = 2), 'bo')\n",
    "plt.xlabel('TE (ms)')\n",
    "plt.ylabel('Signal intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View values in plot above\n",
    "# np.mean(voxel_vals, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View x axis in plot above\n",
    "# TE_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the shapes for all the different parts that go into selecting the right data to fit\n",
    "# np.mean(voxel_vals, axis = 2).shape, motion_select.shape, TE_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the fitting\n",
    "y = np.mean(voxel_vals, axis = 2)[motion_select == 0] # average bright voxels\n",
    "x = np.tile(TE_vec, (N_meas, 1)).transpose()[motion_select == 0] # TE_vec reshaped for the data above\n",
    "\n",
    "d = {'TE': np.tile(TE_vec, (N_meas, 1)).transpose()[motion_select == 0],\n",
    "   'SI': np.mean(voxel_vals, axis = 2)[motion_select == 0],\n",
    "   'ln_SI': np.log(np.mean(voxel_vals, axis = 2)[motion_select == 0])}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "# This is overkill for a simple linear regression, but good practice at a universal way to do regression analysis\n",
    "reg = smf.ols('ln_SI ~ TE', data = df)\n",
    "res = reg.fit()\n",
    "\n",
    "# standard deviation of residuals (SDR)\n",
    "SSE = res.ssr\n",
    "SDR = np.sqrt(SSE/(len(x)-1))\n",
    "print('SDR = ', SDR)\n",
    "\n",
    "# T2 confidence intervals\n",
    "res_ci = res.conf_int().loc['TE'].tolist()\n",
    "T2_ci = [res_ci[0]**-1]\n",
    "T2_ci.append(res_ci[1]**-1)\n",
    "\n",
    "# plot of data and fit\n",
    "df_pred = pd.DataFrame(data = {'TE': np.linspace(min(x), max(x), 100)})\n",
    "df_pred['ln_SI'] = res.predict(df_pred.TE) # NOTE: the predict method must operate on a df once the formula api is used\n",
    "df_pred['SI'] = np.exp(df_pred.ln_SI)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.TE,df.SI,'.', label=\"Data\")\n",
    "ax.plot(df_pred.TE,df_pred.SI,'-', label=\"Fit\")\n",
    "ax.legend(loc = 'best')\n",
    "plt.ylabel('SI')\n",
    "plt.xlabel('TE')\n",
    "plt.savefig(os.path.join(outdir, fnamebase + 'plt_data_fit.png'))\n",
    "\n",
    "# plot of linearized data and fit\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.TE, df.ln_SI, '.', label=\"Data\")\n",
    "ax.plot(df_pred.TE, df_pred.ln_SI, '-', label=\"Fit\")\n",
    "ax.legend(loc = 'best')\n",
    "plt.ylabel('ln(SI)')\n",
    "plt.xlabel('TE')\n",
    "plt.savefig(os.path.join(outdir, fnamebase + 'plt_data_fit_lin.png'))\n",
    "\n",
    "# normal residuals?\n",
    "sm.qqplot(res.resid)\n",
    "\n",
    "# plot of studentized residuals\n",
    "infl = res.get_influence()\n",
    "plt.figure()\n",
    "plt.plot(df.TE, infl.resid_studentized_external, 'o', label='studentized_external')\n",
    "plt.plot(df.TE, infl.resid_studentized_internal, 'x', label='studentized_internal')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel('TE')\n",
    "plt.savefig(os.path.join(outdir, fnamebase + 'studentized_res.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate corrected C, corrected T2 and SO2 for Hct\n",
    "C = -1/res.params.TE\n",
    "T2 = (T1_blood**-1 - res.params.TE)**-1\n",
    "print('C =', C, 'T2 =', T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import T2toSvO2\n",
    "model = 'neonate_liu' \n",
    "SvO2 = T2toSvO2.T2toSvO2(T2, Hct, tCPMG = 10, model = model)\n",
    "print('SO2 =', float(SvO2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results\n",
    "if write_results:\n",
    "    import csv  \n",
    "\n",
    "    # field names  \n",
    "    fields = ['subject', 'T2', 'Hct', 'SvO2', 'conversion_method', 'T2CI-', 'T2CI+', 'SDR', 'motion_select']  \n",
    "\n",
    "    # data rows of csv file  \n",
    "    row = [fnamebase[:-1], T2, Hct, float(SvO2), model, -T2_ci[0], -T2_ci[1], float(SDR), motion_select]\n",
    "\n",
    "    # name of csv file  \n",
    "    filename = os.path.join(results_dir, 'TRUST_analysis_results.csv')\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        write_fields = True\n",
    "\n",
    "    # writing to csv file  \n",
    "    with open(filename, 'a', newline='') as file:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(file)  \n",
    "\n",
    "        if write_fields:\n",
    "            # writing the fields  \n",
    "            csvwriter.writerow(fields)  \n",
    "\n",
    "        # writing the data rows  \n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "    print('Finished.\\n results written to:', filename)\n",
    "else:\n",
    "    print('Finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [fnamebase[:-1], T2, Hct, float(SvO2), model, -T2_ci[0], -T2_ci[1], float(SDR), motion_select]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unravel_index hint\n",
    "# note that np is like C for indexing\n",
    "#   | 0      1      2      3\n",
    "# --+------------------------\n",
    "# 0 | 0      1      2      3\n",
    "# 1 | 4      5      6      7\n",
    "# 2 | 8      9     10     11\n",
    "\n",
    "np.unravel_index(6, (3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
